{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8tinZOUlDER"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# FIT5196 Task 2 in Assessment 1\n",
    "    \n",
    "#### Student Name: xxxxxxx\n",
    "#### Student ID: xxxxxxxxx\n",
    "\n",
    "Date: xxxxxxxx\n",
    "\n",
    "Environment: xxxxxx\n",
    "\n",
    "Libraries used:\n",
    "* os (for interacting with the operating system, included in Python xxxx) \n",
    "* pandas 1.1.0 (for dataframe, installed and imported) \n",
    "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
    "* itertools (for performing operations on iterables)\n",
    "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
    "* nltk.tokenize (for tokenization, installed and imported)\n",
    "* nltk.stem (for stemming the tokens, installed and imported)\n",
    "\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnLnFnLlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Input File](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Genegrate numerical representation](#whetev1) <br>\n",
    "[5. Writing Output Files](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
    "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mo6PPRlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewZrff73lDEV"
   },
   "source": [
    "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSr_kwKclDEV"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acwZw2NklDEW"
   },
   "source": [
    "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
    "\n",
    "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to work with dataframes\n",
    "* **multiprocessing:** to perform processes on multi cores for fast performance \n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "qgmGWs8HlDEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwNp0KnWlDEX"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SA7fSJiRlDEY"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bPCuEl8smTHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 4)\n",
      "(124, 5)\n",
      "                                 gmap_id  \\\n",
      "0  0x54cde72868493ca5:0x3bed623bec4c43e3   \n",
      "1  0x808327b71461d6bb:0x7e1165558a98f648   \n",
      "2  0x8083ff64283d3aab:0x278c940d94dac24d   \n",
      "3  0x8084481ecb286ce3:0x9621a5cee38c4ec2   \n",
      "4   0x80844820b85359e3:0xff414747a10c915   \n",
      "\n",
      "                                             reviews             earliest  \\\n",
      "0  [{'user_id': '105000072505849868207', 'time': ...  2011-02-04 08:53:51   \n",
      "1  [{'user_id': '107657303729293182646', 'time': ...  2017-03-19 03:42:30   \n",
      "2  [{'user_id': '102483909621418254915', 'time': ...  2010-11-22 08:52:43   \n",
      "3  [{'user_id': '103560060345361578357', 'time': ...  2013-08-01 09:19:36   \n",
      "4  [{'user_id': '107735448526067964417', 'time': ...  2013-08-13 10:06:50   \n",
      "\n",
      "                latest                                        review_text  \n",
      "0  2021-04-21 20:51:39  was really good.   the pizza, cheese bread and...  \n",
      "1  2021-06-18 00:16:39  really cool place.. very clean with kind peopl...  \n",
      "2  2021-04-20 00:45:20  this place has always been awesome! very frien...  \n",
      "3  2021-08-21 08:04:27  transco is always professional and great to wo...  \n",
      "4  2021-05-22 07:37:44  was there just before opening at 8:45 am.  wen...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"task1_30.json\")\n",
    "df = df.T.reset_index() \n",
    "df.columns = ['gmap_id', 'reviews', 'earliest', 'latest'] \n",
    "\n",
    "print(df.shape)\n",
    "def extract_reviews(df):\n",
    "    df = df[df['reviews'].apply(len) >= 70].reset_index(drop=True)\n",
    "    df['review_text'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        review_texts = []\n",
    "        for review in row['reviews']:\n",
    "            text = review.get('review_text', '').lower()\n",
    "            # text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "            review_texts.append(text)\n",
    "        df.at[index, 'review_text'] = ' '.join(review_texts)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "df_review = extract_reviews(df)\n",
    "print(df_review.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CJDLDI6lDEY"
   },
   "source": [
    "Let's examine what is the content of the file. For this purpose, .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RyLbqkRxmCEZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'really', 'good', 'the', 'pizza', 'cheese', 'bread', 'and', 'cinnamon', 'bread', 'were', 'all', 'excellent', 'it', 's', 'okay', 'but', 'there', 'seems', 'to', 'be', 'less', 'toppings', 'on', 'the', 'pizza', 'the', 'last', 'time', 'we', 'were', 'there', 'they', 'didn', 't', 'have', 'dressings', 'for', 'their', 'salads', 'the', 'employees', 'are', 'great', 'though', 'because', 'this', 'place', 'is', 'great', 'dylan', 'has', 'thee', 'best', 'customer', 'service', 'i', 'believe', 'i', 've', 'ever', 'came', 'across', 'the', 'pizza', 'is', 'great', 'every', 'time', 'well', 'worth', 'the', 'visit', 'online', 'ordering', 'and', 'in', 'store', 'pickup', 'is', 'fast', 'and', 'easy', 'love', 'the', 'pizza', 'combinations', 'and', 'great', 'food', 'staff', 'is', 'always', 'polite', 'my', 'experience', 'last', 'night', 'was', 'less', 'than', 'preferable', 'went', 'in', 'today', 'and', 'the', 'manager', 'made', 'it', 'right', 'by', 'me', 'love', 'this', 'place', 'very', 'well', 'kept', 'and', 'welcoming', 'spot', 'they', 'serve', 'large', 'portions', 'and', 'for', 'affordable', 'prices', 'i', 'enjoyed', 'the', 'meals', 'very', 'much', 'and', 'the', 'waiters', 'were', 'very', 'amiable', 'and', 'chatty', 'i', 'recommend', 'this', 'place', 'this', 'place', 'has', 'the', 'most', 'awesome', 'crew', 'always', 'friendly', 'knowledgeable', 'quick', 'and', 'efficient', 'the', 'menu', 'is', 'great', 'my', 'favorite', 'i', 'll', 'get', 'heck', 'for', 'it', 'i', 'm', 'sure', 'the', 'hawaiian', 'with', 'creamy', 'garlic', 'sauce', 'instead', 'of', 'red', 'sauce', 'i', 've', 'always', 'received', 'superb', 'service', 'and', 'if', 'by', 'chance', 'someone', 'doesn', 't', 'i', 'm', 'sorry', 'but', 'i', 'highly', 'recommend', 'this', 'restaurant', 'ps', 'take', 'n', 'bake', 'means', 'you', 'have', 'to', 'cook', 'awesome', 'costumer', 'service', 'great', 'place', 'to', 'get', 'a', 'pizza', 'and', 'feed', 'all', 'the', 'kids', 'at', 'a', 'reasonable', 'price', 'it', 'takes', 'about', 'min', 'for', 'them', 'to', 'prepare', 'your', 'order', 'so', 'you', 'have', 'to', 'make', 'sure', 'to', 'keep', 'that', 'in', 'mind', 'when', 'organizing', 'a', 'pizza', 'order', 'they', 'offer', 'daily', 'and', 'weekly', 'specials', 'which', 'is', 'always', 'nice', 'we', 'like', 'a', 'great', 'deal', 'they', 'even', 'offer', 'cookie', 'dough', 'a', 'fresh', 'salads', 'to', 'add', 'to', 'your', 'pizza', 'my', 'favorite', 'is', 'the', 'chicken', 'bacon', 'artichoke', 'pizza', 'greatest', 'place', 'around', 'the', 'employees', 'especially', 'dylan', 'are', 'incredibly', 'helpful', 'and', 'always', 'smiling', 'always', 'consistent', 'with', 'weekly', 'deals', 'pizzas', 'are', 'good', 'even', 'better', 'because', 'they', 'are', 'cooked', 'in', 'your', 'own', 'oven', 'this', 'place', 'is', 'great', 'the', 'staff', 'is', 'fast', 'and', 'the', 'manager', 'is', 'top', 'notch', 'great', 'price', 'great', 'food', 'stars', 'all', 'the', 'way', 'the', 'pizza', 'is', 'good', 'fast', 'service', 'good', 'pizza', 'and', 'friendly', 'staff', 'papa', 'murphy', 's', 'is', 'our', 'favorite', 'way', 'to', 'do', 'pizza', 'like', 'to', 'the', 'selection', 'around', 'ability', 'to', 'customize', 'service', 'is', 'usually', 'faster', 'than', 'promised', 'love', 'it', 'great', 'place', 'for', 'quick', 'and', 'easy', 'take', 'and', 'bake', 'pizza', 'the', 'staff', 'are', 'young', 'and', 'pleasant', 'i', 'have', 'been', 'to', 'quite', 'a', 'few', 'papa', 'murphy', 's', 'but', 'this', 'one', 'is', 'definitely', 'the', 'best', 'friendly', 'crew', 'that', 'are', 'very', 'welcoming', 'and', 'work', 'very', 'quickly', 'and', 'promptly', 'and', 'the', 'salads', 'were', 'surprisingly', 'good', 'we', 'get', 'pizza', 'and', 'salad', 'weekly', 'from', 'here', 'this', 'location', 'by', 'far', 'is', 'the', 'worst', 'i', 'guess', 'when', 'there', 'are', 'not', 'many', 'options', 'around', 'you', 'can', 'get', 'away', 'with', 'selling', 'thrown', 'together', 'slop', 'new', 'management', 'and', 'customer', 'service', 'would', 'go', 'a', 'long', 'way', 'here', 'always', 'great', 'service', 'and', 'bargains', 'great', 'cheap', 'pizza', 'i', 'would', 'give', 'them', 'higher', 'rating', 'if', 'they', 'could', 'manage', 'to', 'have', 'my', 'order', 'ready', 'on', 'time', 'every', 'single', 'time', 'they', 'are', 'at', 'least', 'ten', 'minutes', 'behind', 'if', 'it', 'wasn', 't', 'such', 'a', 'great', 'deal', 'i', 'would', 'never', 'go', 'back', 'these', 'folks', 'are', 'amazing', 'fun', 'and', 'make', 'the', 'best', 'pizza', 'plus', 'they', 'have', 'gluten', 'free', 'options', 'that', 'are', 'superb', 'we', 'went', 'in', 'at', 'this', 'morning', 'we', 'were', 'the', 'first', 'ones', 'in', 'the', 'door', 'they', 'didn', 't', 'listen', 'about', 'what', 'was', 'wanted', 'on', 'the', 'pizza', 'they', 'were', 'rude', 'we', 'asked', 'for', 'the', 'tuesday', 'we', 'asked', 'for', 'extra', 'cheese', 'and', 'got', 'ignored', 'we', 'left', 'for', 'mins', 'at', 'the', 'most', 'went', 'back', 'to', 'the', 'store', 'and', 'saw', 'someone', 'else', 'coming', 'out', 'with', 'his', 'pizza', 'when', 'we', 'got', 'back', 'into', 'the', 'store', 'i', 'was', 'saying', 'how', 'we', 'were', 'there', 'first', 'and', 'how', 'come', 'our', 'pizza', 'wasn', 't', 'even', 'ready', 'and', 'waiting', 'i', 'thought', 'that', 'papa', 'murphy', 's', 'was', 'supposed', 'to', 'be', 'a', 'great', 'take', 'and', 'bake', 'the', 'people', 'who', 'work', 'there', 'suck', 'always', 'friendly', 'staff', 'good', 'ready', 'to', 'bake', 'pizzas', 'so', 'convenient', 'it', 'is', 'a', 'treat', 'to', 'have', 'a', 'piping', 'hot', 'pizza', 'from', 'your', 'own', 'oven', 'that', 'is', 'ready', 'when', 'you', 'are', 'they', 'have', 'delicious', 'pizza', 'of', 'course', 'but', 'the', 'half', 'a', 'have', 'cheese', 'and', 'spinach', 'with', 'mushrooms', 'yumminess', 'great', 'food', 'on', 'vacation', 'good', 'pizza', 'very', 'good', 'pizza', 'and', 'very', 'consistent', 'i', 've', 'always', 'had', 'a', 'good', 'experience', 'there', 'enjoy', 'chatting', 'with', 'nice', 'employees', 'while', 'waiting', 'and', 'the', 'pizza', 'is', 'always', 'great', 'the', 'service', 'here', 'was', 'sub', 'par', 'at', 'the', 'best', 'we', 'called', 'a', 'and', 'made', 'an', 'order', 'for', 'our', 'pizza', 'we', 'were', 'told', 'it', 'would', 'be', 'ready', 'in', 'min', 'the', 'online', 'time', 'says', 'the', 'store', 'closes', 'at', 'so', 'we', 'stopped', 'and', 'got', 'gas', 'first', 'as', 'we', 'arrive', 'it', 'is', 'mind', 'you', 'i', 'say', 'can', 'i', 'get', 'my', 'pizza', 'they', 'tell', 'me', 'sorry', 'we', 'r', 'closed', 'long', 'story', 'short', 'fix', 'the', 'times', 'online', 'today', 'they', 'were', 'a', 'little', 'confused', 'in', 'the', 'store', 'seemed', 'like', 'everyone', 'was', 'a', 'new', 'employee', 'getting', 'orders', 'confused', 'i', 'm', 'hoping', 'our', 'next', 'experience', 'is', 'better', 'i', 'love', 'this', 'place', 'and', 'would', 'love', 'to', 'come', 'back', 'as', 'long', 'as', 'stuff', 'gets', 'worked', 'out', 'picked', 'up', 'some', 'pizzas', 'always', 'great', 'service', 'and', 'good', 'pizza', 'staff', 'is', 'always', 'pleasant', 'orders', 'from', 'phone', 'are', 'most', 'always', 'ready', 'for', 'you', 'if', 'you', 'call', 'in', 'or', 'order', 'online', 'service', 'is', 'faster', 'service', 'is', 'usually', 'pretty', 'good', 'and', 'friendly', 'their', 'service', 'is', 'great', 'the', 'best', 'is', 'when', 'you', 'order', 'online', 'they', 'are', 'prompt', 'and', 'very', 'friendly', 'great', 'customer', 'service', 'from', 'rachel', 'she', 'greeted', 'us', 'as', 'we', 'walked', 'in', 'and', 'told', 'us', 'the', 'specials', 'while', 'she', 'kept', 'pace', 'filling', 'the', 'order', 'she', 'was', 'working', 'on', 'when', 'we', 'came', 'in', 'rachel', 'was', 'friendly', 'and', 'personable', 'and', 'had', 'a', 'wonderful', 'attitude', 'the', 'store', 'was', 'clean', 'and', 'the', 'premise', 'case', 'was', 'full', 'cheap', 'pizza', 'not', 'always', 'the', 'greatest', 'pay', 'more', 'and', 'get', 'better', 'it', 's', 'pizza', 'in', 'mt', 'shasta', 'u', 'may', 'or', 'may', 'not', 'get', 'heart', 'burn', 'but', 'it', 's', 'worth', 'the', 'gamble', 'am', 'i', 'right', 'great', 'food', 'and', 'service', 'always', 'have', 'order', 'ready', 'on', 'time', 'when', 'ordering', 'online', 'pleasant', 'staff', 'gluten', 'free', 'pizza', 'options', 'as', 'well', 'as', 'salads', 'cheese', 'breads', 'and', 'even', 'various', 'cookie', 'doughs', 'best', 'place', 'to', 'make', 'a', 'cheap', 'version', 'of', 'round', 'table', 's', 'italian', 'garlic', 'supreme', 'worst', 'pizza', 'i', 'ever', 'had', 'the', 'thin', 'crust', 'was', 'not', 'even', 'like', 'pizza', 'crust', 'the', 'best', 'take', 'and', 'bake', 'pizza', 'in', 'town', 'great', 'place', 'to', 'get', 'a', 'take', 'and', 'bake', 'and', 'good', 'customer', 'service', 'quick', 'serve', 'and', 'a', 'good', 'product', 'what', 'isn', 't', 'to', 'love', 'about', 'a', 'wrll', 'made', 'pizza', 'to', 'take', 'home', 'and', 'cook', 'i', 'always', 'get', 'good', 'service', 'here', 'good', 'pizza', 'inexpensive', 'the', 'manager', 'is', 'great', 'food', 'is', 'good', 'fast', 'i', 'mean', 'it', 's', 'papa', 'murphy', 's', 'delicious', 'and', 'cheap', 'and', 'they', 'get', 'their', 'veggies', 'from', 'propacific', 'cheap', 'and', 'delicious', 'home', 'baked', 'pizzas', 'friendly', 'staff', 'quality', 'and', 'care', 'has', 'gone', 'downhill', 'unfortunately', 'very', 'good', 'pizza', 'for', 'the', 'price', 'great', 'prices', 'love', 'the', 'tuesday', 'special', 'good', 'pizza', 'always', 'ordered', 'over', 'the', 'phone', 'and', 'always', 'had', 'my', 'pizza', 'ready', 'by', 'the', 'time', 'i', 'got', 'there', 'always', 'did', 'special', 'orders', 'too', 'and', 'never', 'missed', 'an', 'order', 'item', 'once', 'oder', 'before', 'you', 'get', 'their', 'or', 'don', 't', 'be', 'surprised', 'to', 'wait', 'minutes', 'because', 'others', 'ordered', 'over', 'the', 'phone', 'ahead', 'of', 'you', 'friendly', 'fast', 'service', 'i', 'love', 'love', 'love', 'love', 'love', 'love', 'this', 'place', 'bomb', 'love', 'take', 'n', 'bake', 'love', 'there', 'pizza', 'didn', 't', 'know', 'that', 'you', 'had', 'to', 'bake', 'it', 'i', 'was', 'camping', 'fresh', 'ingredients', 'always', 'good', 'yummy', 'so', 'dirty', 'i', 'didn', 't', 'want', 'the', 'nice', 'pizza', 's', 'it', 'is', 'take', 'and', 'bake', 'what', 'else', 'can', 'i', 'say', 'its', 'always', 'good', 'pizza', 'just', 'go', 'through', 'the', 'street', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpDVyW4YlDEZ"
   },
   "source": [
    "It is noteiced that file contains ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7c1-c3olDEa"
   },
   "source": [
    "Having parsed the pdf file, the following observations can be made: ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0yElwywlDEa"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SA37sl0lDEa"
   },
   "outputs": [],
   "source": [
    "#some code to download paper files if needed.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ENnHWjoXlDEc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9esGMx8lDEc"
   },
   "source": [
    "In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsnRR2c4lDEc"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uh9oUXAlDEd"
   },
   "source": [
    "Let's examine the dictionary generated. For counting the total number of reviews extracted ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUZuFeuQlDEd"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VIfQCD1VlDEe"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchByyjolDEf"
   },
   "source": [
    "Tokenization is a principal step in text processing and producing unigrams. In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "p8zT4N0RlDEf"
   },
   "outputs": [],
   "source": [
    "#some code\n",
    "for index,row in df_review.iterrows():\n",
    "    # Tokenization\n",
    "    review_text = row[\"review_text\"]\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    unigram_tokens = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # Remove tokens less than three\n",
    "    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n",
    "\n",
    "    # Retokenize for multi word\n",
    "    bigrams = ngrams(unigram_tokens, n = 2)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "    fdbigram.most_common()\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(unigram_tokens)\n",
    "    bigram_token = finder.nbest(bigram_measures.pmi, 201)\n",
    "    tokenizer = MWETokenizer(bigram_token)\n",
    "    mwe_tokens = tokenizer.tokenize(unigram_tokens)\n",
    "    # print(mwe_tokens)\n",
    "\n",
    "    # Remove independent stop words\n",
    "    stopwords_list = []\n",
    "    with open(r'Student Data/stopwords_en.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            stopwords_list.append(line.strip())\n",
    "    stopped_tokens = [w for w in mwe_tokens if w not in stopwords_list]\n",
    "\n",
    "    # Stem words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "\n",
    "    df_review.at[index, \"tokennized_review\"] = stemed_tokens\n",
    "\n",
    "\n",
    "# print(df_review.head())\n",
    "\n",
    "# Remove context dependent and rare word\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_context_dependent_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses > 0.95 or len(business_set) / total_businesses < 0.05]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    cleaned_tokens = [w for w in row['tokennized_review'] if w not in rare_context_dependent_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = cleaned_tokens\n",
    "\n",
    "# print(df_review.head())\n",
    "\n",
    "# Export word count txt file\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "unique_vocab = set(all_tokens)\n",
    "sorted_unique_vocab = sorted(unique_vocab)\n",
    "with open(\"test_vocab.txt\",'w') as file:\n",
    "    for index,word in enumerate(sorted_unique_vocab):\n",
    "        file.write(f\"{word}:{index}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZos1q6lDEf"
   },
   "source": [
    "The above operation results in a dictionary with PID representing keys and a single string for all reviews of the day concatenated to each other. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPBNTTq6lDEg"
   },
   "outputs": [],
   "source": [
    "#Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVqFfwwMlDEg"
   },
   "source": [
    "At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NglwwiJRnPZd"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.2. Whatever else <a class=\"anchor\" name=\"whetev\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ve6IZ2I-lDEg"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.3. Generate numerical representation<a class=\"anchor\" name=\"bigrams\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGhUY2UlDEg"
   },
   "source": [
    "One of the tasks is to generate the numerical representation for all tokens in abstract.  ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgFFtm6qlDEg"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpy1-tQalDEg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cbSKT6PlDEg"
   },
   "source": [
    "Random descriptions and justification ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hDrmwEylDEg"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO1PJO-dlDEh"
   },
   "source": [
    "At this stage, we have a dictionary of tokenized words, whose keys are indicative of....\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfMYJ0XdlDEh"
   },
   "source": [
    "#### Whatever else <a class=\"anchor\" name=\"whatev1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmaGJYIJlDEl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMBqRetlDEl"
   },
   "source": [
    "files need to be generated:\n",
    "* Vocabulary list\n",
    "* Sparse matrix (count_vectors)\n",
    "\n",
    "This is performed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc6tQ4ljlDEm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDlbpGYilDEm"
   },
   "source": [
    "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6OUXHlxlDEm"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkGH81YFlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtxqUAwmlDEn"
   },
   "source": [
    "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__n1fdIqlDEn"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUFQU-QXlDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWjri6x_lDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAprlSblDEn"
   },
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXYKxO8lDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HppxDtWNlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCkWr-M1lDEo"
   },
   "source": [
    "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9O-a1UlDEo"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
