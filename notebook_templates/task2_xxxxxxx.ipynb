{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8tinZOUlDER"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# FIT5196 Task 2 in Assessment 1\n",
    "    \n",
    "#### Student Name: xxxxxxx\n",
    "#### Student ID: xxxxxxxxx\n",
    "\n",
    "Date: xxxxxxxx\n",
    "\n",
    "Environment: xxxxxx\n",
    "\n",
    "Libraries used:\n",
    "* os (for interacting with the operating system, included in Python xxxx) \n",
    "* pandas 1.1.0 (for dataframe, installed and imported) \n",
    "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
    "* itertools (for performing operations on iterables)\n",
    "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
    "* nltk.tokenize (for tokenization, installed and imported)\n",
    "* nltk.stem (for stemming the tokens, installed and imported)\n",
    "\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnLnFnLlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Input File](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Genegrate numerical representation](#whetev1) <br>\n",
    "[5. Writing Output Files](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
    "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mo6PPRlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewZrff73lDEV"
   },
   "source": [
    "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSr_kwKclDEV"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acwZw2NklDEW"
   },
   "source": [
    "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
    "\n",
    "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to work with dataframes\n",
    "* **multiprocessing:** to perform processes on multi cores for fast performance \n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qgmGWs8HlDEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwNp0KnWlDEX"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SA7fSJiRlDEY"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bPCuEl8smTHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  gmap_id  \\\n",
      "0   0x54cde72868493ca5:0x3bed623bec4c43e3   \n",
      "1   0x54d400a3503bf3cb:0x6f54a8d273995ade   \n",
      "2   0x54d4ecda5872a3d3:0x133d4cc056e83ab4   \n",
      "3   0x808327b71461d6bb:0x7e1165558a98f648   \n",
      "4   0x8083ff64283d3aab:0x278c940d94dac24d   \n",
      "5   0x8084481ecb286ce3:0x9621a5cee38c4ec2   \n",
      "6    0x80844820b85359e3:0xff414747a10c915   \n",
      "7   0x8084d14c6ecc3abf:0x8516c9daf203a890   \n",
      "8   0x808506ba36069ef1:0x872c89de33e28e2c   \n",
      "9   0x808515c134667051:0x9a164a19ad59e8ff   \n",
      "10  0x808515d26b90f18f:0x1e763dbe37ae7516   \n",
      "11  0x8085613c0ba3453d:0xafafb2f005ef4730   \n",
      "12  0x8085619759ff8759:0xae65b3fcdfd02e45   \n",
      "13  0x808567674283bbdd:0x1b039c22a6be8de5   \n",
      "14   0x808577a4077796b7:0x74c2df8373dd019   \n",
      "15  0x808577b06c49fbcb:0x3c48448b009b80d3   \n",
      "16  0x808579239e005685:0x85d07736600cdbc6   \n",
      "17  0x80857dcdecda8877:0xc56ed45ede87f983   \n",
      "18  0x808580e9b3089847:0x3a964d5d97defd44   \n",
      "19  0x8085ac3b9e99ebf5:0xfcba33f78ee94975   \n",
      "\n",
      "                                              reviews             earliest  \\\n",
      "0   [{'user_id': '105000072505849868207', 'time': ...  2011-02-04 08:53:51   \n",
      "1   [{'user_id': '117801025993772751929', 'time': ...  2014-12-27 15:01:27   \n",
      "2   [{'user_id': '110831597999913201409', 'time': ...  2016-01-19 10:16:32   \n",
      "3   [{'user_id': '107657303729293182646', 'time': ...  2017-03-19 03:42:30   \n",
      "4   [{'user_id': '102483909621418254915', 'time': ...  2010-11-22 08:52:43   \n",
      "5   [{'user_id': '103560060345361578357', 'time': ...  2013-08-01 09:19:36   \n",
      "6   [{'user_id': '107735448526067964417', 'time': ...  2013-08-13 10:06:50   \n",
      "7   [{'user_id': '110602798780788249509', 'time': ...  2015-08-22 04:29:16   \n",
      "8   [{'user_id': '117907132945489566754', 'time': ...  2011-02-28 03:41:00   \n",
      "9   [{'user_id': '111337015314703849717', 'time': ...  2016-07-17 10:52:08   \n",
      "10  [{'user_id': '109345904128747867524', 'time': ...  2015-04-03 04:54:56   \n",
      "11  [{'user_id': '104842787028508296184', 'time': ...  2016-02-07 21:48:16   \n",
      "12  [{'user_id': '116968081707930145178', 'time': ...  2015-03-13 10:01:12   \n",
      "13  [{'user_id': '110831739193632254097', 'time': ...  2018-07-21 14:27:12   \n",
      "14  [{'user_id': '106791093812668613355', 'time': ...  2019-01-22 10:35:19   \n",
      "15  [{'user_id': '113994618120666216331', 'time': ...  2017-07-12 03:54:30   \n",
      "16  [{'user_id': '104942316924186030230', 'time': ...  2013-01-20 10:39:57   \n",
      "17  [{'user_id': '100911988369194938362', 'time': ...  2008-02-23 11:00:00   \n",
      "18  [{'user_id': '112990815040274497235', 'time': ...  2013-12-02 13:21:58   \n",
      "19  [{'user_id': '100071540331351820457', 'time': ...  2017-09-16 08:55:23   \n",
      "\n",
      "                 latest  \n",
      "0   2021-04-21 20:51:39  \n",
      "1   2021-03-31 00:41:05  \n",
      "2   2021-05-07 14:36:57  \n",
      "3   2021-06-18 00:16:39  \n",
      "4   2021-04-20 00:45:20  \n",
      "5   2021-08-21 08:04:27  \n",
      "6   2021-05-22 07:37:44  \n",
      "7   2021-05-25 03:59:07  \n",
      "8   2021-05-27 11:30:48  \n",
      "9   2019-08-11 15:04:39  \n",
      "10  2021-05-02 14:40:47  \n",
      "11  2021-06-13 03:09:20  \n",
      "12  2021-05-15 04:02:29  \n",
      "13  2021-06-15 15:31:16  \n",
      "14  2021-07-05 15:19:48  \n",
      "15  2018-09-01 13:59:47  \n",
      "16  2021-05-10 08:28:45  \n",
      "17  2021-05-18 13:46:47  \n",
      "18  2021-05-22 10:53:57  \n",
      "19  2021-07-07 02:22:53  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"task1_30.json\")\n",
    "df = df.T.reset_index() \n",
    "df.columns = ['gmap_id', 'reviews', 'earliest', 'latest'] \n",
    "print(df.head(20))\n",
    "for index, row in df.iterrows():\n",
    "    filtered_reviews = [review for review in row[\"reviews\"] if review.get(\"review_text\") != \"None\"]\n",
    "    df.at[index, \"reviews\"] = filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: After reading the json file into a pandas dataframe. I found that the column and rows need to be transposed. Hence I used .T to ahieve that. Only the review contains more than 70 texts review is needed. So I used a list comprehension first to drop the review that is \"None\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ENnHWjoXlDEc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9esGMx8lDEc"
   },
   "source": [
    "In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YsnRR2c4lDEc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 4)\n",
      "(124, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "def extract_reviews(df):\n",
    "    df = df[df['reviews'].apply(len) >= 70].reset_index(drop=True)\n",
    "    df['review_text'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        review_texts = []\n",
    "        for review in row['reviews']:\n",
    "            text = review.get('review_text', '').lower()\n",
    "            # text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "            review_texts.append(text)\n",
    "        df.at[index, 'review_text'] = ' '.join(review_texts)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "df_review = extract_reviews(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uh9oUXAlDEd"
   },
   "source": [
    "After dropping all the review_text which is \"None\", I filtered the review by whose \"reviews\" list has more than 70 elements by applying len() method. After filtering the reviews, I created a new dataframe to store them. After that, a for loop is used to iterate through each row of the new data frame to store the review text to a new column \"review_text\". And then, join the review_text list to a string for further tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VIfQCD1VlDEe"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchByyjolDEf"
   },
   "source": [
    "Tokenization is a principal step in text processing and producing unigrams. In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct version\n",
    "stopwords_list = []\n",
    "with open(r'Student Data/stopwords_en.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        stopwords_list.append(line.strip())\n",
    "df_review[\"tokennized_review\"] = None\n",
    "for index,row in df_review.iterrows():\n",
    "    # Tokenization\n",
    "    review_text = row[\"review_text\"]\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    unigram_tokens = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # Remove tokens less than three\n",
    "    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n",
    "\n",
    "    # Remove independent stop words\n",
    "    stopped_tokens = [w for w in unigram_tokens if w not in stopwords_list]\n",
    "    #Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "    df_review.at[index, \"tokennized_review\"] = stemed_tokens\n",
    "    # print(mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZos1q6lDEf"
   },
   "source": [
    "1. Read the stopwords txt and append them to a list called stopwords_lis\n",
    "2. Iterate through the rows of df_reviews\n",
    "3. Task a finished: Tokenize the review_text of each row by RegexpTokenizer\n",
    "Justification: To perform further processing like remove stop words, Stemmentization etc, Words need to be tokenized first.\n",
    "4. Task e finished: Used list comprehension to  filter out the tokens to be length longer than3\n",
    "Justification: Words's Lenth less than 3 are always meanningless words. Remove than at the beginning can help improve efficency of data processing.\n",
    "5. Task b1 finishied: Used list comprehension to filter the unigram_tokens by word not in the stop_word list.\n",
    "Justification: Stop words are considered noise that needs to be removed. Remove them before stemmatization, because if the stopwords are stemmed first, the words would be transformed that not being able to match the stopword list.\n",
    "6. Stemming the remaining tokens to their root forms using the Porter Stemmer.\n",
    "Justification: To identify context dependent stopwords, words need to be stemmed before removing them\n",
    "\n",
    "Explanation on result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OPBNTTq6lDEg"
   },
   "outputs": [],
   "source": [
    "# Remove context dependent stopwords\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "context_dependent_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses > 0.95]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    stopped_tokens = [w for w in row['tokennized_review'] if w not in context_dependent_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = stopped_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Identifying and removing context-dependent stopwords, which are words that appear in more than 95% of businesses.\n",
    "Justification: For example: \"The words \"restaurant\" and \"restaurants\" are treated separately. \"Restaurant\" might appear in 80% of reviews, and \"restaurants\" in 20%. Individually, neither might exceed a frequency threshold (like 95%). Thus, neither is removed as a context-dependent stopword.\n",
    "\n",
    "Explanation on result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_words = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses < 0.05]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    rared_tokens = [w for w in row['tokennized_review'] if w not in rare_words]\n",
    "    df_review.at[index, \"tokennized_review\"] = rared_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The purpose of this code is to remove rare words from the tokenized reviews. Rare words are defined as those that appear in less than 5% of the businesses in the dataset.\n",
    "Justification: This step should be done after stemmatization. For example,\"running,\" \"ran,\" and \"runs\" all stem to \"run.\" If rare word removal were done before stemming, each form would be treated as a separate entity, potentially missing the fact that collectively they might not be rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram words\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "bigram_token = finder.nbest(bigram_measures.pmi, 200)  # Get the top 200 bigrams\n",
    "tokenizer = MWETokenizer(bigram_token)\n",
    "for index, row in df_review.iterrows():\n",
    "    mwe_tokens = tokenizer.tokenize(row[\"tokennized_review\"])\n",
    "    df_review.at[index, \"tokennized_review\"] = mwe_tokens  \n",
    "# print(df_review.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Used MWETokenizer with pmi to find the first 200 bigrams and iterate through each row to identify the bigram and add bigram to it.\n",
    "\n",
    "Justification: The Pointwise Mutual Information (PMI) measure used to identify meaningful bigrams relies on the frequency and co-occurrence of words within a text. By removing unimportant or noisy words first, the PMI calculation becomes more accurate and reflective of meaningful word associations. If bigrams were generated earlier, PMI scores could be skewed by the presence of irrelevant or overly common tokens, leading to less meaningful bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export word count txt file\n",
    "all_tokens_per_doc = [token for token in df_review[\"tokennized_review\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sorted the cleaned tokens and write them to test_vocab.txt by desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVqFfwwMlDEg"
   },
   "source": [
    "At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NglwwiJRnPZd"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.2. Whatever else <a class=\"anchor\" name=\"whetev\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ve6IZ2I-lDEg"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.3. Generate numerical representation<a class=\"anchor\" name=\"bigrams\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGhUY2UlDEg"
   },
   "source": [
    "One of the tasks is to generate the numerical representation for all tokens in abstract.  ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "sgFFtm6qlDEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abil': 0, 'absolut': 1, 'accept': 2, 'access': 3, 'accessori': 4, 'accid': 5, 'accommod': 6, 'account': 7, 'accur': 8, 'acknowledg': 9, 'act': 10, 'action': 11, 'activ': 12, 'actual': 13, 'ad': 14, 'add': 15, 'addict': 16, 'addit': 17, 'address': 18, 'adequ': 19, 'adjust': 20, 'admit': 21, 'admit_purpos': 22, 'ador': 23, 'adult': 24, 'advanc': 25, 'advantag': 26, 'advertis': 27, 'advic': 28, 'advis': 29, 'afford': 30, 'afraid': 31, 'afternoon': 32, 'age': 33, 'agenc': 34, 'agent': 35, 'aggress': 36, 'ago': 37, 'agre': 38, 'ahead': 39, 'air': 40, 'allow': 41, 'alot': 42, 'alright': 43, 'altern': 44, 'amaz': 45, 'amazingli': 46, 'ambianc': 47, 'ambienc': 48, 'amd': 49, 'amd_helpful': 50, 'amen': 51, 'america': 52, 'american': 53, 'amount': 54, 'ampl': 55, 'angel': 56, 'anim': 57, 'annoy': 58, 'answer': 59, 'anticip': 60, 'anxieti': 61, 'anymor': 62, 'anytim': 63, 'apolog': 64, 'apolog_inconveni': 65, 'apologet': 66, 'app': 67, 'appar': 68, 'appear': 69, 'appet': 70, 'appetit': 71, 'appl': 72, 'appli': 73, 'applic': 74, 'appoint': 75, 'appreci': 76, 'approach': 77, 'approv': 78, 'approxim': 79, 'approxim_monthli': 80, 'appt': 81, 'april': 82, 'april_april': 83, 'area': 84, 'aren': 85, 'argu': 86, 'arm': 87, 'arrang': 88, 'arriv': 89, 'art': 90, 'asada': 91, 'asap': 92, 'asian': 93, 'ask': 94, 'aspect': 95, 'ass': 96, 'assess': 97, 'asset': 98, 'assist': 99, 'associ': 100, 'assort': 101, 'assort_scene': 102, 'assum': 103, 'assur': 104, 'ate': 105, 'atmospher': 106, 'attach': 107, 'attach_video': 108, 'attempt': 109, 'attend': 110, 'attent': 111, 'attitud': 112, 'attorney': 113, 'attract': 114, 'authent': 115, 'author': 116, 'author_submit': 117, 'auto': 118, 'automat': 119, 'avail': 120, 'ave': 121, 'averag': 122, 'avocado': 123, 'avoid': 124, 'aw': 125, 'awar': 126, 'awesom': 127, 'awhil': 128, 'awsom': 129, 'babi': 130, 'back': 131, 'bacon': 132, 'bad': 133, 'bag': 134, 'bake': 135, 'balanc': 136, 'ball': 137, 'band': 138, 'bang': 139, 'bang_buck': 140, 'bank': 141, 'bar': 142, 'bare': 143, 'bartend': 144, 'base': 145, 'basic': 146, 'bathroom': 147, 'batteri': 148, 'bay': 149, 'bbq': 150, 'beach': 151, 'bean': 152, 'beat': 153, 'beauti': 154, 'bed': 155, 'beef': 156, 'beer': 157, 'began': 158, 'begin': 159, 'behalf': 160, 'behavior': 161, 'believ': 162, 'bell': 163, 'belli': 164, 'benefit': 165, 'bet': 166, 'beverag': 167, 'bewar': 168, 'big': 169, 'bigger': 170, 'biggest': 171, 'bike': 172, 'bill': 173, 'birthday': 174, 'bit': 175, 'bite': 176, 'black': 177, 'blame': 178, 'bland': 179, 'blast': 180, 'blend': 181, 'bless': 182, 'block': 183, 'blood': 184, 'blow': 185, 'blown': 186, 'blue': 187, 'blvd': 188, 'blvd_nut': 189, 'board': 190, 'bodi': 191, 'bomb': 192, 'bone': 193, 'bonu': 194, 'book': 195, 'booth': 196, 'bore': 197, 'bore_task': 198, 'bother': 199, 'bottl': 200, 'bottom': 201, 'bought': 202, 'bowl': 203, 'box': 204, 'boy': 205, 'boyfriend': 206, 'brand': 207, 'bread': 208, 'break': 209, 'breakfast': 210, 'breath': 211, 'breez': 212, 'brew': 213, 'bright': 214, 'brilliant': 215, 'bring': 216, 'broke': 217, 'broken': 218, 'brother': 219, 'brought': 220, 'brown': 221, 'brunch': 222, 'brush': 223, 'brush_coat': 224, 'brush_shower': 225, 'brush_wave': 226, 'btw': 227, 'buck': 228, 'bud': 229, 'buddi': 230, 'buddi_mine': 231, 'budget': 232, 'bug': 233, 'bug_remedi': 234, 'build': 235, 'built': 236, 'bump': 237, 'bump_bump': 238, 'bun': 239, 'bunch': 240, 'burger': 241, 'burn': 242, 'burnt': 243, 'burrito': 244, 'busi': 245, 'butt': 246, 'butter': 247, 'buy': 248, 'cake': 249, 'cali': 250, 'cali_fav': 251, 'california': 252, 'call': 253, 'calm': 254, 'camera': 255, 'can': 256, 'cancel': 257, 'candi': 258, 'cap': 259, 'cap_cap': 260, 'car': 261, 'caramel': 262, 'card': 263, 'care': 264, 'carn': 265, 'carn_asada': 266, 'carnita': 267, 'carri': 268, 'cart': 269, 'case': 270, 'cash': 271, 'cashier': 272, 'casual': 273, 'cat': 274, 'catch': 275, 'cater': 276, 'caught': 277, 'caus': 278, 'celebr': 279, 'cell': 280, 'cent': 281, 'center': 282, 'central': 283, 'central_coast': 284, 'central_valley': 285, 'chain': 286, 'chair': 287, 'challeng': 288, 'chanc': 289, 'chang': 290, 'charact': 291, 'charg': 292, 'chase': 293, 'chase_asap': 294, 'chat': 295, 'cheap': 296, 'cheaper': 297, 'cheapest': 298, 'cheat': 299, 'check': 300, 'cheer': 301, 'chees': 302, 'chef': 303, 'chewi': 304, 'chicken': 305, 'child': 306, 'children': 307, 'chile': 308, 'chile_lime': 309, 'chili': 310, 'chill': 311, 'chines': 312, 'chip': 313, 'chipotl': 314, 'chocol': 315, 'choic': 316, 'choos': 317, 'chose': 318, 'chosen': 319, 'chri': 320, 'christma': 321, 'cilantro': 322, 'cinnamon': 323, 'citi': 324, 'claim': 325, 'class': 326, 'classi': 327, 'classic': 328, 'clean': 329, 'cleaner': 330, 'cleanli': 331, 'clear': 332, 'clerk': 333, 'client': 334, 'clinic': 335, 'clock': 336, 'close': 337, 'closer': 338, 'closest': 339, 'cloth': 340, 'club': 341, 'clue': 342, 'coast': 343, 'coat': 344, 'coat_tight': 345, 'coat_tongu': 346, 'cocktail': 347, 'coconut': 348, 'code': 349, 'code_enforc': 350, 'coffe': 351, 'coke': 352, 'cold': 353, 'colleagu': 354, 'collect': 355, 'collect_agenc': 356, 'color': 357, 'combin': 358, 'combo': 359, 'come': 360, 'comfort': 361, 'commend': 362, 'comment': 363, 'commerci': 364, 'commit': 365, 'common': 366, 'commun': 367, 'compani': 368, 'companion': 369, 'companion_octob': 370, 'compar': 371, 'comparison': 372, 'comparison_competitor': 373, 'compass': 374, 'compassion': 375, 'compet': 376, 'competit': 377, 'competitor': 378, 'complain': 379, 'complaint': 380, 'complet': 381, 'complex': 382, 'complic': 383, 'complic_mama': 384, 'compliment': 385, 'complimentari': 386, 'complimentari_profil': 387, 'comput': 388, 'con': 389, 'concern': 390, 'condiment': 391, 'condit': 392, 'confid': 393, 'confirm': 394, 'confus': 395, 'connect': 396, 'consid': 397, 'consid_fals': 398, 'consider': 399, 'consist': 400, 'constantli': 401, 'consult': 402, 'consum': 403, 'contact': 404, 'contain': 405, 'content': 406, 'content_condiment': 407, 'content_eventu': 408, 'continu': 409, 'contract': 410, 'control': 411, 'conveni': 412, 'convers': 413, 'cook': 414, 'cooki': 415, 'cooki_dough': 416, 'cool': 417, 'cordial': 418, 'corn': 419, 'corner': 420, 'corpor': 421, 'correct': 422, 'correctli': 423, 'cost': 424, 'costco': 425, 'costum': 426, 'couldn': 427, 'couldnt': 428, 'count': 429, 'counter': 430, 'counti': 431, 'countri': 432, 'coupl': 433, 'coupon': 434, 'court': 435, 'courteou': 436, 'courtesi': 437, 'cover': 438, 'covid': 439, 'cow': 440, 'cowork': 441, 'cozi': 442, 'crack': 443, 'craft': 444, 'crave': 445, 'crazi': 446, 'cream': 447, 'creat': 448, 'creativ': 449, 'credit': 450, 'crew': 451, 'cri': 452, 'crisp': 453, 'crispi': 454, 'crook': 455, 'cross': 456, 'crowd': 457, 'crunch': 458, 'crunchi': 459, 'crust': 460, 'cuisin': 461, 'cultur': 462, 'cultur_languag': 463, 'cup': 464, 'current': 465, 'curri': 466, 'custom': 467, 'cut': 468, 'cute': 469, 'cuz': 470, 'dad': 471, 'daili': 472, 'damag': 473, 'damn': 474, 'daniel': 475, 'dark': 476, 'darn': 477, 'data': 478, 'date': 479, 'daughter': 480, 'david': 481, 'day': 482, 'dead': 483, 'deadlin': 484, 'deal': 485, 'dealer': 486, 'dealership': 487, 'dealt': 488, 'death': 489, 'debit': 490, 'decad': 491, 'decent': 492, 'decid': 493, 'decis': 494, 'decor': 495, 'dedic': 496, 'deduct': 497, 'deep': 498, 'def': 499, 'defin': 500, 'definit': 501, 'degre': 502, 'del': 503, 'delay': 504, 'delic': 505, 'delici': 506, 'delight': 507, 'delish': 508, 'deliv': 509, 'deliveri': 510, 'demand': 511, 'demeanor': 512, 'demeanor_mama': 513, 'deni': 514, 'depart': 515, 'depend': 516, 'dept': 517, 'describ': 518, 'descript': 519, 'desert': 520, 'deserv': 521, 'design': 522, 'design_specialist': 523, 'desir': 524, 'desk': 525, 'desper': 526, 'dessert': 527, 'destroy': 528, 'destroy_admit': 529, 'detail': 530, 'determin': 531, 'develop': 532, 'develop_bump': 533, 'develop_poorli': 534, 'devic': 535, 'diagnos': 536, 'dial': 537, 'diamond': 538, 'didn': 539, 'didnt': 540, 'die': 541, 'diego': 542, 'differ': 543, 'difficult': 544, 'dilig': 545, 'dine': 546, 'dinner': 547, 'dip': 548, 'direct': 549, 'directli': 550, 'dirt': 551, 'dirti': 552, 'disabl': 553, 'disappoint': 554, 'discount': 555, 'discov': 556, 'discuss': 557, 'disgust': 558, 'dish': 559, 'dislik': 560, 'dislik_fake': 561, 'display': 562, 'disrespect': 563, 'dissatisfi': 564, 'distanc': 565, 'divers': 566, 'dmv': 567, 'doctor': 568, 'document': 569, 'doesn': 570, 'doesnt': 571, 'dog': 572, 'doggi': 573, 'dollar': 574, 'don': 575, 'dont': 576, 'door': 577, 'doubl': 578, 'doubt': 579, 'dough': 580, 'downsid': 581, 'downsid_unexpect': 582, 'downtown': 583, 'dozen': 584, 'drain': 585, 'draw': 586, 'dream': 587, 'dress': 588, 'dri': 589, 'drink': 590, 'drive': 591, 'driver': 592, 'drop': 593, 'drove': 594, 'drug': 595, 'drug_addict': 596, 'drug_heck': 597, 'dude': 598, 'due': 599, 'dump': 600, 'dump_station': 601, 'duti': 602, 'duti_seek': 603, 'ear': 604, 'earli': 605, 'earlier': 606, 'earn': 607, 'earth': 608, 'eas': 609, 'easi': 610, 'easier': 611, 'easiest': 612, 'easiest_strang': 613, 'easili': 614, 'east': 615, 'east_coast': 616, 'eat': 617, 'eaten': 618, 'ect': 619, 'edg': 620, 'edit': 621, 'educ': 622, 'effect': 623, 'effici': 624, 'effort': 625, 'egg': 626, 'elderli': 627, 'email': 628, 'embarrass': 629, 'emerg': 630, 'employe': 631, 'empti': 632, 'enchilada': 633, 'encount': 634, 'encourag': 635, 'end': 636, 'energi': 637, 'enforc': 638, 'engag': 639, 'engin': 640, 'english': 641, 'enhanc': 642, 'enjoy': 643, 'ensur': 644, 'enter': 645, 'entertain': 646, 'enthusiast': 647, 'enthusiast_dedic': 648, 'entir': 649, 'entranc': 650, 'entre': 651, 'environ': 652, 'epic': 653, 'equal': 654, 'equal_delish': 655, 'equip': 656, 'error': 657, 'error_code': 658, 'essenti': 659, 'establish': 660, 'estim': 661, 'ethic': 662, 'evalu': 663, 'even': 664, 'event': 665, 'eventu': 666, 'everyday': 667, 'everytim': 668, 'exact': 669, 'exam': 670, 'examin': 671, 'exceed': 672, 'excel': 673, 'except': 674, 'excess': 675, 'excess_unnecessari': 676, 'exchang': 677, 'excit': 678, 'excus': 679, 'execut': 680, 'exel': 681, 'exist': 682, 'expect': 683, 'expens': 684, 'experi': 685, 'experienc': 686, 'expert': 687, 'expertis': 688, 'expir': 689, 'explain': 690, 'explan': 691, 'explor': 692, 'express': 693, 'extend': 694, 'extens': 695, 'extens_web': 696, 'extra': 697, 'extraordinari': 698, 'extrem': 699, 'eye': 700, 'fabul': 701, 'face': 702, 'facil': 703, 'fact': 704, 'factor': 705, 'fail': 706, 'fair': 707, 'fairli': 708, 'fake': 709, 'fall': 710, 'fall_bone': 711, 'fals': 712, 'fals_advertis': 713, 'famili': 714, 'familiar': 715, 'familiar_restor': 716, 'famou': 717, 'fan': 718, 'fanci': 719, 'fantast': 720, 'fashion': 721, 'fast': 722, 'faster': 723, 'fastest': 724, 'fat': 725, 'father': 726, 'fault': 727, 'fav': 728, 'favor': 729, 'favorit': 730, 'fear': 731, 'featur': 732, 'fee': 733, 'feed': 734, 'feel': 735, 'fell': 736, 'fellow': 737, 'fellow_vari': 738, 'felt': 739, 'femal': 740, 'fianc': 741, 'field': 742, 'fight': 743, 'figur': 744, 'file': 745, 'fill': 746, 'filter': 747, 'filthi': 748, 'final': 749, 'financi': 750, 'find': 751, 'fine': 752, 'finish': 753, 'fire': 754, 'firm': 755, 'fish': 756, 'fit': 757, 'fix': 758, 'flat': 759, 'flavor': 760, 'flexibl': 761, 'flexibl_realiti': 762, 'fli': 763, 'floor': 764, 'flow': 765, 'focu': 766, 'focus': 767, 'folk': 768, 'follow': 769, 'food': 770, 'fool': 771, 'fool_giant': 772, 'foot': 773, 'forc': 774, 'forev': 775, 'forget': 776, 'forgot': 777, 'form': 778, 'fortun': 779, 'forward': 780, 'found': 781, 'fountain': 782, 'fourth': 783, 'fourth_juli': 784, 'frame': 785, 'francisco': 786, 'freak': 787, 'free': 788, 'freeway': 789, 'freindli': 790, 'french': 791, 'french_toast': 792, 'frequent': 793, 'fresh': 794, 'freshli': 795, 'fri': 796, 'friday': 797, 'friend': 798, 'friendliest': 799, 'front': 800, 'frozen': 801, 'fruit': 802, 'frustrat': 803, 'fulfil': 804, 'full': 805, 'fulli': 806, 'fun': 807, 'function': 808, 'fund': 809, 'funni': 810, 'fur': 811, 'fur_babi': 812, 'futur': 813, 'ga': 814, 'gain': 815, 'gal': 816, 'gal_surround': 817, 'game': 818, 'garag': 819, 'garbag': 820, 'garlic': 821, 'gate': 822, 'gather': 823, 'gaug': 824, 'gave': 825, 'gem': 826, 'gener': 827, 'gentl': 828, 'gentleman': 829, 'gentlemen': 830, 'genuin': 831, 'giant': 832, 'gift': 833, 'girl': 834, 'girlfriend': 835, 'give': 836, 'glad': 837, 'gladli': 838, 'glass': 839, 'glove': 840, 'gluten': 841, 'god': 842, 'god_bless': 843, 'gold': 844, 'gold_stuf': 845, 'golden': 846, 'golden_rule': 847, 'gonna': 848, 'goodi': 849, 'googl': 850, 'gorgeou': 851, 'gotta': 852, 'gourmet': 853, 'grab': 854, 'grace': 855, 'graciou': 856, 'grade': 857, 'grand': 858, 'grate': 859, 'greas': 860, 'greasi': 861, 'greatest': 862, 'greatli': 863, 'green': 864, 'greet': 865, 'grew': 866, 'grill': 867, 'gross': 868, 'ground': 869, 'group': 870, 'grow': 871, 'guacamol': 872, 'guarante': 873, 'guard': 874, 'guess': 875, 'guest': 876, 'guid': 877, 'guidanc': 878, 'guy': 879, 'hadn': 880, 'hair': 881, 'half': 882, 'hamburg': 883, 'hand': 884, 'handi': 885, 'handl': 886, 'hang': 887, 'happen': 888, 'happi': 889, 'happier': 890, 'happili': 891, 'hard': 892, 'hasn': 893, 'hassl': 894, 'hate': 895, 'haven': 896, 'head': 897, 'headach': 898, 'health': 899, 'healthi': 900, 'hear': 901, 'heard': 902, 'heart': 903, 'hearti': 904, 'heat': 905, 'heaven': 906, 'heavenli': 907, 'heavi': 908, 'heavi_lift': 909, 'heck': 910, 'hectic': 911, 'held': 912, 'hell': 913, 'hella': 914, 'hella_freak': 915, 'help': 916, 'helpful': 917, 'hesit': 918, 'hey': 919, 'hidden': 920, 'hidden_gem': 921, 'high': 922, 'higher': 923, 'highest': 924, 'highli': 925, 'highlight': 926, 'hill': 927, 'hip': 928, 'hip_hip': 929, 'hire': 930, 'histori': 931, 'hit': 932, 'hold': 933, 'hole': 934, 'hole_wall': 935, 'holiday': 936, 'home': 937, 'homeless': 938, 'homemad': 939, 'honest': 940, 'honesti': 941, 'honesti_superior': 942, 'honestli': 943, 'honor': 944, 'hood': 945, 'hood_crook': 946, 'hook': 947, 'hope': 948, 'horribl': 949, 'hospit': 950, 'host': 951, 'hot': 952, 'hotel': 953, 'hour': 954, 'hous': 955, 'hr': 956, 'huge': 957, 'human': 958, 'humbl': 959, 'humor': 960, 'hundr': 961, 'hung': 962, 'hungri': 963, 'hunt': 964, 'hurri': 965, 'hurt': 966, 'husband': 967, 'hype': 968, 'ice': 969, 'idea': 970, 'ideal': 971, 'identifi': 972, 'identifi_sourc': 973, 'ignor': 974, 'ill': 975, 'imagin': 976, 'immedi': 977, 'impati': 978, 'import': 979, 'importantli': 980, 'imposs': 981, 'impress': 982, 'improv': 983, 'in': 984, 'in_approxim': 985, 'inch': 986, 'includ': 987, 'incompet': 988, 'inconsist': 989, 'inconveni': 990, 'incorrect': 991, 'increas': 992, 'incred': 993, 'individu': 994, 'indoor': 995, 'industri': 996, 'inexpens': 997, 'info': 998, 'inform': 999, 'ingredi': 1000, 'initi': 1001, 'injuri': 1002, 'inquir': 1003, 'insan': 1004, 'insid': 1005, 'insist': 1006, 'insist_shave': 1007, 'inspect': 1008, 'instal': 1009, 'instruct': 1010, 'insult': 1011, 'insur': 1012, 'integr': 1013, 'intens': 1014, 'interact': 1015, 'interest': 1016, 'interior': 1017, 'intern': 1018, 'internet': 1019, 'introduc': 1020, 'inventori': 1021, 'invest': 1022, 'invest_sake': 1023, 'invit': 1024, 'involv': 1025, 'isn': 1026, 'issu': 1027, 'item': 1028, 'ive': 1029, 'jack': 1030, 'jame': 1031, 'jerk': 1032, 'jerk_tongu': 1033, 'job': 1034, 'joe': 1035, 'john': 1036, 'join': 1037, 'joint': 1038, 'joke': 1039, 'jose': 1040, 'josh': 1041, 'joy': 1042, 'juic': 1043, 'juici': 1044, 'juli': 1045, 'jump': 1046, 'june': 1047, 'keep': 1048, 'key': 1049, 'kick': 1050, 'kick_ass': 1051, 'kick_butt': 1052, 'kid': 1053, 'kill': 1054, 'kill_appetit': 1055, 'kind': 1056, 'kinda': 1057, 'kindli': 1058, 'king': 1059, 'kitchen': 1060, 'knew': 1061, 'knock': 1062, 'know': 1063, 'knowledg': 1064, 'kudo': 1065, 'lab': 1066, 'labor': 1067, 'labor_complic': 1068, 'lack': 1069, 'ladi': 1070, 'laid': 1071, 'lake': 1072, 'land': 1073, 'languag': 1074, 'larg': 1075, 'larger': 1076, 'late': 1077, 'latest': 1078, 'latest_technolog': 1079, 'laugh': 1080, 'law': 1081, 'lazi': 1082, 'lead': 1083, 'leak': 1084, 'learn': 1085, 'leas': 1086, 'leav': 1087, 'left': 1088, 'leftov': 1089, 'leg': 1090, 'legal': 1091, 'legit': 1092, 'lemon': 1093, 'length': 1094, 'let': 1095, 'letter': 1096, 'lettuc': 1097, 'lettuc_tomato': 1098, 'level': 1099, 'licens': 1100, 'lie': 1101, 'life': 1102, 'lifetim': 1103, 'lifetim_transpar': 1104, 'lift': 1105, 'lift_accessori': 1106, 'lift_remark': 1107, 'light': 1108, 'like': 1109, 'lil': 1110, 'lime': 1111, 'limit': 1112, 'line': 1113, 'list': 1114, 'listen': 1115, 'lit': 1116, 'liter': 1117, 'live': 1118, 'lo': 1119, 'lo_angel': 1120, 'load': 1121, 'loan': 1122, 'lobbi': 1123, 'local': 1124, 'locat': 1125, 'lock': 1126, 'lol': 1127, 'long': 1128, 'longer': 1129, 'look': 1130, 'loop': 1131, 'loop_seamless': 1132, 'loos': 1133, 'lose': 1134, 'loss': 1135, 'lost': 1136, 'lot': 1137, 'loud': 1138, 'lousi': 1139, 'love': 1140, 'lover': 1141, 'low': 1142, 'lower': 1143, 'lowest': 1144, 'lowest_humor': 1145, 'loyal': 1146, 'luck': 1147, 'lucki': 1148, 'luckili': 1149, 'lui': 1150, 'lunch': 1151, 'machin': 1152, 'mad': 1153, 'made': 1154, 'magic': 1155, 'magic_gather': 1156, 'magnific': 1157, 'mail': 1158, 'main': 1159, 'maintain': 1160, 'mainten': 1161, 'major': 1162, 'make': 1163, 'mall': 1164, 'mama': 1165, 'mama_mama': 1166, 'man': 1167, 'manag': 1168, 'mango': 1169, 'manner': 1170, 'map': 1171, 'march': 1172, 'mari': 1173, 'mark': 1174, 'market': 1175, 'mask': 1176, 'massiv': 1177, 'massiv_remodel': 1178, 'master': 1179, 'master_craft': 1180, 'match': 1181, 'matter': 1182, 'mayo': 1183, 'mcdonald': 1184, 'mcdonald_ect': 1185, 'meal': 1186, 'mean': 1187, 'meant': 1188, 'meant_arrang': 1189, 'meant_fellow': 1190, 'measur': 1191, 'meat': 1192, 'mechan': 1193, 'med': 1194, 'medic': 1195, 'mediocr': 1196, 'medium': 1197, 'medium_rare': 1198, 'meet': 1199, 'melt': 1200, 'melt_mouth': 1201, 'member': 1202, 'memor': 1203, 'memori': 1204, 'men': 1205, 'mention': 1206, 'menu': 1207, 'merced': 1208, 'mess': 1209, 'messag': 1210, 'met': 1211, 'metal': 1212, 'metal_admit': 1213, 'method': 1214, 'mexican': 1215, 'michael': 1216, 'mid': 1217, 'middl': 1218, 'midnight': 1219, 'mike': 1220, 'mile': 1221, 'militari': 1222, 'milk': 1223, 'million': 1224, 'min': 1225, 'mind': 1226, 'mine': 1227, 'mini': 1228, 'mini_split': 1229, 'minim': 1230, 'minimum': 1231, 'minor': 1232, 'mint': 1233, 'minu': 1234, 'minu_score': 1235, 'minut': 1236, 'miss': 1237, 'mistak': 1238, 'mix': 1239, 'mobil': 1240, 'model': 1241, 'modern': 1242, 'mom': 1243, 'moment': 1244, 'monday': 1245, 'money': 1246, 'monitor': 1247, 'month': 1248, 'monthli': 1249, 'mood': 1250, 'morn': 1251, 'mother': 1252, 'mouth': 1253, 'move': 1254, 'movi': 1255, 'movi_score': 1256, 'multipl': 1257, 'mushroom': 1258, 'music': 1259, 'nacho': 1260, 'nail': 1261, 'name': 1262, 'napkin': 1263, 'napkin_condiment': 1264, 'nasti': 1265, 'natur': 1266, 'navig': 1267, 'nearbi': 1268, 'neat': 1269, 'neck': 1270, 'need': 1271, 'needless': 1272, 'neg': 1273, 'negoti': 1274, 'neighbor': 1275, 'neighbor_quit': 1276, 'neighborhood': 1277, 'nerv': 1278, 'nerv_blame': 1279, 'nerv_caught': 1280, 'nerv_rack': 1281, 'nervou': 1282, 'news': 1283, 'news_greas': 1284, 'nicest': 1285, 'night': 1286, 'nightmar': 1287, 'noisi': 1288, 'noodl': 1289, 'noon': 1290, 'normal': 1291, 'north': 1292, 'nose': 1293, 'notch': 1294, 'note': 1295, 'notic': 1296, 'notifi': 1297, 'number': 1298, 'numer': 1299, 'nurs': 1300, 'nut': 1301, 'obtain': 1302, 'obviou': 1303, 'occas': 1304, 'occasion': 1305, 'octob': 1306, 'octob_hurt': 1307, 'odd': 1308, 'offer': 1309, 'offic': 1310, 'offici': 1311, 'offici_joy': 1312, 'oil': 1313, 'older': 1314, 'omg': 1315, 'onion': 1316, 'onion_ring': 1317, 'onlin': 1318, 'open': 1319, 'oper': 1320, 'opinion': 1321, 'opportun': 1322, 'opposit': 1323, 'option': 1324, 'orang': 1325, 'orang_counti': 1326, 'order': 1327, 'organ': 1328, 'orient': 1329, 'origin': 1330, 'outcom': 1331, 'outdoor': 1332, 'outlet': 1333, 'outrag': 1334, 'outstand': 1335, 'overcharg': 1336, 'overcook': 1337, 'overli': 1338, 'overnight': 1339, 'overpr': 1340, 'overwhelm': 1341, 'owe': 1342, 'own': 1343, 'owner': 1344, 'ownership': 1345, 'pack': 1346, 'packag': 1347, 'pad': 1348, 'paid': 1349, 'pain': 1350, 'painless': 1351, 'paint': 1352, 'pair': 1353, 'pandem': 1354, 'paper': 1355, 'paperwork': 1356, 'par': 1357, 'parent': 1358, 'park': 1359, 'part': 1360, 'parti': 1361, 'partner': 1362, 'partner_butt': 1363, 'partner_task': 1364, 'pass': 1365, 'passion': 1366, 'past': 1367, 'path': 1368, 'pathet': 1369, 'patienc': 1370, 'patient': 1371, 'patio': 1372, 'patron': 1373, 'pay': 1374, 'payment': 1375, 'peac': 1376, 'peak': 1377, 'penni': 1378, 'peopl': 1379, 'pepper': 1380, 'perfect': 1381, 'perfectli': 1382, 'perform': 1383, 'period': 1384, 'person': 1385, 'personnel': 1386, 'pet': 1387, 'phenomen': 1388, 'phone': 1389, 'photo': 1390, 'physic': 1391, 'physic_exam': 1392, 'pick': 1393, 'picki': 1394, 'picki_grew': 1395, 'pickl': 1396, 'pickup': 1397, 'pictur': 1398, 'piec': 1399, 'pinch': 1400, 'pinch_nerv': 1401, 'pink': 1402, 'pipe': 1403, 'pizza': 1404, 'plain': 1405, 'plan': 1406, 'plant': 1407, 'plastic': 1408, 'plastic_contain': 1409, 'plate': 1410, 'play': 1411, 'plaza': 1412, 'pleas': 1413, 'pleasant': 1414, 'pleasantli': 1415, 'pleasantli_surpris': 1416, 'pleasur': 1417, 'plenti': 1418, 'pocket': 1419, 'pocket_visibl': 1420, 'point': 1421, 'polici': 1422, 'polit': 1423, 'poor': 1424, 'poorli': 1425, 'pop': 1426, 'popular': 1427, 'pork': 1428, 'pork_belli': 1429, 'portion': 1430, 'posit': 1431, 'possibl': 1432, 'post': 1433, 'pot': 1434, 'potato': 1435, 'potenti': 1436, 'pound': 1437, 'pound_pound': 1438, 'pour': 1439, 'power': 1440, 'ppl': 1441, 'practic': 1442, 'pre': 1443, 'precaut': 1444, 'prefer': 1445, 'premium': 1446, 'prepar': 1447, 'prescrib': 1448, 'prescript': 1449, 'present': 1450, 'press': 1451, 'press_freak': 1452, 'pressur': 1453, 'pretti': 1454, 'prevent': 1455, 'previou': 1456, 'previous': 1457, 'price': 1458, 'pricey': 1459, 'prici': 1460, 'pride': 1461, 'primari': 1462, 'print': 1463, 'prior': 1464, 'privat': 1465, 'pro': 1466, 'problem': 1467, 'procedur': 1468, 'proceed': 1469, 'process': 1470, 'produc': 1471, 'product': 1472, 'profession': 1473, 'profil': 1474, 'profit': 1475, 'profit_comparison': 1476, 'program': 1477, 'progress': 1478, 'project': 1479, 'promis': 1480, 'promot': 1481, 'prompt': 1482, 'promptli': 1483, 'proof': 1484, 'proof_ownership': 1485, 'proper': 1486, 'properli': 1487, 'properti': 1488, 'protect': 1489, 'protein': 1490, 'protein_destroy': 1491, 'protocol': 1492, 'provid': 1493, 'public': 1494, 'pull': 1495, 'pump': 1496, 'punctual': 1497, 'pup': 1498, 'puppi': 1499, 'purchas': 1500, 'pure': 1501, 'purpos': 1502, 'push': 1503, 'pushi': 1504, 'put': 1505, 'qualifi': 1506, 'qualiti': 1507, 'quarter': 1508, 'quesadilla': 1509, 'question': 1510, 'quick': 1511, 'quicker': 1512, 'quickli': 1513, 'quiet': 1514, 'quit': 1515, 'quot': 1516, 'race': 1517, 'rack': 1518, 'rain': 1519, 'rais': 1520, 'ran': 1521, 'ranch': 1522, 'rang': 1523, 'rapid': 1524, 'rapid_aggress': 1525, 'rapid_avail': 1526, 'rare': 1527, 'rate': 1528, 'rave': 1529, 'raw': 1530, 'ray': 1531, 'reach': 1532, 'read': 1533, 'readi': 1534, 'real': 1535, 'realiti': 1536, 'realiti_movi': 1537, 'realiz': 1538, 'rear': 1539, 'reason': 1540, 'reassur': 1541, 'recal': 1542, 'reccomend': 1543, 'receipt': 1544, 'receiv': 1545, 'recent': 1546, 'recept': 1547, 'receptionist': 1548, 'recip': 1549, 'recogn': 1550, 'recomend': 1551, 'recommend': 1552, 'record': 1553, 'red': 1554, 'reduc': 1555, 'refer': 1556, 'referr': 1557, 'referr_bonu': 1558, 'refil': 1559, 'refresh': 1560, 'refund': 1561, 'refus': 1562, 'regist': 1563, 'regret': 1564, 'regular': 1565, 'regularli': 1566, 'relat': 1567, 'relationship': 1568, 'relationship_qualifi': 1569, 'relax': 1570, 'reli': 1571, 'reli_loyal': 1572, 'reliabl': 1573, 'relief': 1574, 'reliev': 1575, 'remain': 1576, 'remark': 1577, 'remedi': 1578, 'remedi_relationship': 1579, 'rememb': 1580, 'remind': 1581, 'remodel': 1582, 'remov': 1583, 'renov': 1584, 'renov_modern': 1585, 'renov_senior': 1586, 'rent': 1587, 'rental': 1588, 'rep': 1589, 'repair': 1590, 'repeat': 1591, 'repeatedli': 1592, 'replac': 1593, 'repli': 1594, 'report': 1595, 'repres': 1596, 'request': 1597, 'requir': 1598, 'research': 1599, 'reserv': 1600, 'resid': 1601, 'resid_sourc': 1602, 'resolv': 1603, 'respect': 1604, 'respond': 1605, 'respons': 1606, 'rest': 1607, 'restaur': 1608, 'restor': 1609, 'restroom': 1610, 'result': 1611, 'retail': 1612, 'retir': 1613, 'return': 1614, 'review': 1615, 'reward': 1616, 'rib': 1617, 'rice': 1618, 'rich': 1619, 'ride': 1620, 'ridicul': 1621, 'ring': 1622, 'rip': 1623, 'risk': 1624, 'road': 1625, 'roast': 1626, 'rock': 1627, 'roll': 1628, 'room': 1629, 'root': 1630, 'rough': 1631, 'rough_can': 1632, 'rough_impati': 1633, 'roughli': 1634, 'round': 1635, 'routin': 1636, 'row': 1637, 'rude': 1638, 'ruin': 1639, 'rule': 1640, 'run': 1641, 'runner': 1642, 'runner_roughli': 1643, 'rush': 1644, 'sad': 1645, 'sadli': 1646, 'safe': 1647, 'safeti': 1648, 'safeti_protocol': 1649, 'sake': 1650, 'salad': 1651, 'sale': 1652, 'salmon': 1653, 'salsa': 1654, 'salt': 1655, 'salt_caramel': 1656, 'salt_pepper': 1657, 'salti': 1658, 'sampl': 1659, 'san': 1660, 'sandwich': 1661, 'sanit': 1662, 'sanitari': 1663, 'sanitari_filthi': 1664, 'santa': 1665, 'sat': 1666, 'satisfact': 1667, 'satisfi': 1668, 'saturday': 1669, 'sauc': 1670, 'sausag': 1671, 'save': 1672, 'savori': 1673, 'savori_sociabl': 1674, 'scam': 1675, 'scare': 1676, 'scare_death': 1677, 'scari': 1678, 'scene': 1679, 'schedul': 1680, 'school': 1681, 'score': 1682, 'scott': 1683, 'scratch': 1684, 'screen': 1685, 'screw': 1686, 'seafood': 1687, 'seamless': 1688, 'search': 1689, 'season': 1690, 'seat': 1691, 'second': 1692, 'secret': 1693, 'section': 1694, 'secur': 1695, 'seek': 1696, 'select': 1697, 'sell': 1698, 'send': 1699, 'senior': 1700, 'sens': 1701, 'sens_humor': 1702, 'separ': 1703, 'serv': 1704, 'server': 1705, 'set': 1706, 'settl': 1707, 'setup': 1708, 'sevic': 1709, 'shade': 1710, 'shadi': 1711, 'shadi_behavior': 1712, 'shake': 1713, 'shame': 1714, 'shape': 1715, 'share': 1716, 'shave': 1717, 'shell': 1718, 'shelter': 1719, 'shelter_desert': 1720, 'shift': 1721, 'shine': 1722, 'ship': 1723, 'shirt': 1724, 'shock': 1725, 'shoe': 1726, 'shop': 1727, 'short': 1728, 'shortli': 1729, 'shot': 1730, 'shouldn': 1731, 'shout': 1732, 'show': 1733, 'shower': 1734, 'shower_renov': 1735, 'shower_shower': 1736, 'shrimp': 1737, 'shut': 1738, 'sick': 1739, 'side': 1740, 'sign': 1741, 'signific': 1742, 'silver': 1743, 'similar': 1744, 'simpl': 1745, 'simpli': 1746, 'sincer': 1747, 'singl': 1748, 'sister': 1749, 'sit': 1750, 'site': 1751, 'situat': 1752, 'size': 1753, 'skill': 1754, 'skin': 1755, 'skin_shirt': 1756, 'skip': 1757, 'slam': 1758, 'sleep': 1759, 'slice': 1760, 'slightli': 1761, 'slip': 1762, 'slow': 1763, 'small': 1764, 'smaller': 1765, 'smart': 1766, 'smell': 1767, 'smh': 1768, 'smh_crook': 1769, 'smile': 1770, 'smoke': 1771, 'smooth': 1772, 'smoothli': 1773, 'snack': 1774, 'soak': 1775, 'soak_enchilada': 1776, 'sociabl': 1777, 'social': 1778, 'social_distanc': 1779, 'soda': 1780, 'soft': 1781, 'soft_chewi': 1782, 'soggi': 1783, 'sold': 1784, 'solid': 1785, 'solut': 1786, 'solv': 1787, 'son': 1788, 'soo': 1789, 'sooner': 1790, 'sooo': 1791, 'soooo': 1792, 'sort': 1793, 'soul': 1794, 'sound': 1795, 'soup': 1796, 'sour': 1797, 'sourc': 1798, 'south': 1799, 'southern': 1800, 'southern_california': 1801, 'soy': 1802, 'space': 1803, 'spaciou': 1804, 'spanish': 1805, 'spare': 1806, 'speak': 1807, 'special': 1808, 'specialist': 1809, 'specialti': 1810, 'specif': 1811, 'spectacular': 1812, 'speed': 1813, 'speedi': 1814, 'spend': 1815, 'spent': 1816, 'spice': 1817, 'spici': 1818, 'spill': 1819, 'spill_spill': 1820, 'spirit': 1821, 'split': 1822, 'spoke': 1823, 'spoken': 1824, 'sport': 1825, 'spot': 1826, 'spring': 1827, 'squar': 1828, 'squar_foot': 1829, 'squeez': 1830, 'stack': 1831, 'stack_gain': 1832, 'staf': 1833, 'staff': 1834, 'stale': 1835, 'stall': 1836, 'stand': 1837, 'standard': 1838, 'stapl': 1839, 'star': 1840, 'start': 1841, 'state': 1842, 'station': 1843, 'statu': 1844, 'stay': 1845, 'steak': 1846, 'steal': 1847, 'steam': 1848, 'steam_death': 1849, 'steep': 1850, 'steep_pot': 1851, 'stellar': 1852, 'step': 1853, 'stick': 1854, 'stock': 1855, 'stolen': 1856, 'stolen_debit': 1857, 'stomach': 1858, 'stood': 1859, 'stop': 1860, 'store': 1861, 'stori': 1862, 'straight': 1863, 'strang': 1864, 'strawberri': 1865, 'street': 1866, 'stress': 1867, 'strip': 1868, 'strip_mall': 1869, 'strong': 1870, 'strongli': 1871, 'struggl': 1872, 'stuck': 1873, 'student': 1874, 'stuf': 1875, 'stuff': 1876, 'stumbl': 1877, 'stupid': 1878, 'style': 1879, 'submit': 1880, 'success': 1881, 'suck': 1882, 'sudden': 1883, 'suffer': 1884, 'sugar': 1885, 'sugar_coat': 1886, 'suggest': 1887, 'suit': 1888, 'summer': 1889, 'sun': 1890, 'sunday': 1891, 'sunni': 1892, 'super': 1893, 'superb': 1894, 'superior': 1895, 'supervisor': 1896, 'supper': 1897, 'suppli': 1898, 'support': 1899, 'suppos': 1900, 'supposedli': 1901, 'sure': 1902, 'sure_bore': 1903, 'surgeri': 1904, 'surpris': 1905, 'surprisingli': 1906, 'surround': 1907, 'survey': 1908, 'sweet': 1909, 'sweetest': 1910, 'switch': 1911, 'system': 1912, 'tabl': 1913, 'taco': 1914, 'tag': 1915, 'take': 1916, 'takeout': 1917, 'takeout_leftov': 1918, 'talent': 1919, 'talk': 1920, 'tank': 1921, 'tank_gaug': 1922, 'tap': 1923, 'task': 1924, 'task_commend': 1925, 'tast': 1926, 'tasteless': 1927, 'tasti': 1928, 'tax': 1929, 'tea': 1930, 'teach': 1931, 'team': 1932, 'tear': 1933, 'tear_flow': 1934, 'tech': 1935, 'technic': 1936, 'technician': 1937, 'technolog': 1938, 'teeth': 1939, 'tell': 1940, 'temperatur': 1941, 'ten': 1942, 'tend': 1943, 'tender': 1944, 'term': 1945, 'termin': 1946, 'terribl': 1947, 'terrif': 1948, 'test': 1949, 'texa': 1950, 'text': 1951, 'textur': 1952, 'textur_soak': 1953, 'thank': 1954, 'thanksgiv': 1955, 'thick': 1956, 'thier': 1957, 'thier_crook': 1958, 'thier_focu': 1959, 'thin': 1960, 'thin_crust': 1961, 'thing': 1962, 'think': 1963, 'think_runner': 1964, 'tho': 1965, 'thought': 1966, 'thousand': 1967, 'threaten': 1968, 'threw': 1969, 'thrill': 1970, 'throw': 1971, 'thumb': 1972, 'thursday': 1973, 'ticket': 1974, 'tie': 1975, 'tight': 1976, 'til': 1977, 'till': 1978, 'timer': 1979, 'tini': 1980, 'tip': 1981, 'tire': 1982, 'titl': 1983, 'toast': 1984, 'today': 1985, 'toilet': 1986, 'told': 1987, 'tomato': 1988, 'tomato_pathet': 1989, 'tomorrow': 1990, 'ton': 1991, 'tongu': 1992, 'tonight': 1993, 'tool': 1994, 'tooth': 1995, 'top': 1996, 'tortilla': 1997, 'total': 1998, 'touch': 1999, 'tough': 2000, 'tourist': 2001, 'tourist_trap': 2002, 'towel': 2003, 'towel_greas': 2004, 'town': 2005, 'track': 2006, 'trade': 2007, 'tradit': 2008, 'traffic': 2009, 'train': 2010, 'transact': 2011, 'transfer': 2012, 'transit': 2013, 'transit_van': 2014, 'transpar': 2015, 'trap': 2016, 'trash': 2017, 'travel': 2018, 'treat': 2019, 'treatment': 2020, 'tree': 2021, 'tremend': 2022, 'trick': 2023, 'trip': 2024, 'troubl': 2025, 'truck': 2026, 'true': 2027, 'trust': 2028, 'trustworthi': 2029, 'truth': 2030, 'tuck': 2031, 'tuck_squar': 2032, 'tuesday': 2033, 'tune': 2034, 'turn': 2035, 'type': 2036, 'typic': 2037, 'ultim': 2038, 'ultim_buddi': 2039, 'unabl': 2040, 'unbeliev': 2041, 'uncomfort': 2042, 'understaf': 2043, 'understand': 2044, 'understood': 2045, 'unexpect': 2046, 'unfortun': 2047, 'unfortun_chosen': 2048, 'unfriendli': 2049, 'unhappi': 2050, 'unhelp': 2051, 'uniqu': 2052, 'unit': 2053, 'unlik': 2054, 'unlik_competitor': 2055, 'unnecessari': 2056, 'unorgan': 2057, 'unpleas': 2058, 'unprofession': 2059, 'up': 2060, 'upbeat': 2061, 'updat': 2062, 'upfront': 2063, 'upgrad': 2064, 'upsel': 2065, 'upset': 2066, 'usual': 2067, 'utmost': 2068, 'vacat': 2069, 'vacuum': 2070, 'valley': 2071, 'valu': 2072, 'van': 2073, 'vari': 2074, 'vari_greatli': 2075, 'varieti': 2076, 'vega': 2077, 'vegan': 2078, 'veget': 2079, 'vegetarian': 2080, 'veggi': 2081, 'vehicl': 2082, 'venu': 2083, 'verifi': 2084, 'version': 2085, 'version_round': 2086, 'vet': 2087, 'veteran': 2088, 'vibe': 2089, 'video': 2090, 'view': 2091, 'viru': 2092, 'visibl': 2093, 'visit': 2094, 'voic': 2095, 'voic_mail': 2096, 'waist': 2097, 'wait': 2098, 'waiter': 2099, 'waitress': 2100, 'walk': 2101, 'wall': 2102, 'walmart': 2103, 'wanna': 2104, 'want': 2105, 'warm': 2106, 'warmli': 2107, 'warn': 2108, 'warranti': 2109, 'wash': 2110, 'wasn': 2111, 'wasnt': 2112, 'wast': 2113, 'watch': 2114, 'water': 2115, 'wave': 2116, 'way': 2117, 'weak': 2118, 'wear': 2119, 'weather': 2120, 'web': 2121, 'web_applic': 2122, 'websit': 2123, 'wed': 2124, 'wednesday': 2125, 'week': 2126, 'weekday': 2127, 'weekend': 2128, 'weekli': 2129, 'weird': 2130, 'welcom': 2131, 'weren': 2132, 'west': 2133, 'wet': 2134, 'whatsoev': 2135, 'wheel': 2136, 'white': 2137, 'wide': 2138, 'wife': 2139, 'wifi': 2140, 'wifi_competitor': 2141, 'win': 2142, 'window': 2143, 'windshield': 2144, 'windshield_sevic': 2145, 'wine': 2146, 'wing': 2147, 'winter': 2148, 'wipe': 2149, 'wire': 2150, 'wise': 2151, 'wish': 2152, 'wit': 2153, 'woman': 2154, 'women': 2155, 'won': 2156, 'wonder': 2157, 'wont': 2158, 'wood': 2159, 'wood_lake': 2160, 'word': 2161, 'wore': 2162, 'wore_mask': 2163, 'work': 2164, 'worker': 2165, 'workout': 2166, 'world': 2167, 'worri': 2168, 'wors': 2169, 'worst': 2170, 'worth': 2171, 'worthi': 2172, 'wouldn': 2173, 'wouldnt': 2174, 'wound': 2175, 'wow': 2176, 'wrap': 2177, 'write': 2178, 'written': 2179, 'wrong': 2180, 'yard': 2181, 'yeah': 2182, 'year': 2183, 'yell': 2184, 'yellow': 2185, 'yelp': 2186, 'yesterday': 2187, 'young': 2188, 'younger': 2189, 'younger_lil': 2190, 'yr': 2191, 'yum': 2192, 'yummi': 2193, 'zone': 2194}\n"
     ]
    }
   ],
   "source": [
    "#some code\n",
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "documents = [' '.join(tokens) for tokens in all_tokens_per_doc]\n",
    "\n",
    "data_features = vectorizer.fit_transform(documents)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "vocab_dist = {word: index for index, word in enumerate(vocab)}\n",
    "with open(\"test_vocab.txt\",'w') as file:\n",
    "    for index,word in enumerate(vocab_dist):\n",
    "        file.write(f\"{word}:{ index}\\n\")\n",
    "\n",
    "print(vocab_dist)\n",
    "output_countvec = []\n",
    "for doc_index, gmap_id in enumerate(df_review[\"gmap_id\"]):\n",
    "    doc_vector = data_features[doc_index]\n",
    "    token_counts = zip(doc_vector.indices, doc_vector.data)\n",
    "    formatted_counts = [f\"{index}:{frequency}\" for index,frequency in token_counts] \n",
    "    output_line = f\"{gmap_id} \" + \", \".join(formatted_counts)\n",
    "    output_countvec.append(output_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cbSKT6PlDEg"
   },
   "source": [
    "Random descriptions and justification ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO1PJO-dlDEh"
   },
   "source": [
    "At this stage, we have a dictionary of tokenized words, whose keys are indicative of....\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfMYJ0XdlDEh"
   },
   "source": [
    "#### Whatever else <a class=\"anchor\" name=\"whatev1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmaGJYIJlDEl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"countvec_test.txt\",\"w\") as file:\n",
    "    for line in output_countvec:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMBqRetlDEl"
   },
   "source": [
    "files need to be generated:\n",
    "* Vocabulary list\n",
    "* Sparse matrix (count_vectors)\n",
    "\n",
    "This is performed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc6tQ4ljlDEm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDlbpGYilDEm"
   },
   "source": [
    "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Y6OUXHlxlDEm"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkGH81YFlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtxqUAwmlDEn"
   },
   "source": [
    "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "__n1fdIqlDEn"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUFQU-QXlDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWjri6x_lDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAprlSblDEn"
   },
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXYKxO8lDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HppxDtWNlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCkWr-M1lDEo"
   },
   "source": [
    "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9O-a1UlDEo"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
