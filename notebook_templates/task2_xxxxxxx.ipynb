{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8tinZOUlDER"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# FIT5196 Task 2 in Assessment 1\n",
    "    \n",
    "#### Student Name: xxxxxxx\n",
    "#### Student ID: xxxxxxxxx\n",
    "\n",
    "Date: xxxxxxxx\n",
    "\n",
    "Environment: xxxxxx\n",
    "\n",
    "Libraries used:\n",
    "* os (for interacting with the operating system, included in Python xxxx) \n",
    "* pandas 1.1.0 (for dataframe, installed and imported) \n",
    "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
    "* itertools (for performing operations on iterables)\n",
    "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
    "* nltk.tokenize (for tokenization, installed and imported)\n",
    "* nltk.stem (for stemming the tokens, installed and imported)\n",
    "\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnLnFnLlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Input File](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Genegrate numerical representation](#whetev1) <br>\n",
    "[5. Writing Output Files](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
    "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mo6PPRlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewZrff73lDEV"
   },
   "source": [
    "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSr_kwKclDEV"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acwZw2NklDEW"
   },
   "source": [
    "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
    "\n",
    "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to work with dataframes\n",
    "* **multiprocessing:** to perform processes on multi cores for fast performance \n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgmGWs8HlDEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwNp0KnWlDEX"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SA7fSJiRlDEY"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bPCuEl8smTHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 4)\n",
      "(124, 5)\n",
      "                                 gmap_id  \\\n",
      "0  0x54cde72868493ca5:0x3bed623bec4c43e3   \n",
      "1  0x808327b71461d6bb:0x7e1165558a98f648   \n",
      "2  0x8083ff64283d3aab:0x278c940d94dac24d   \n",
      "3  0x8084481ecb286ce3:0x9621a5cee38c4ec2   \n",
      "4   0x80844820b85359e3:0xff414747a10c915   \n",
      "\n",
      "                                             reviews             earliest  \\\n",
      "0  [{'user_id': '105000072505849868207', 'time': ...  2011-02-04 08:53:51   \n",
      "1  [{'user_id': '107657303729293182646', 'time': ...  2017-03-19 03:42:30   \n",
      "2  [{'user_id': '102483909621418254915', 'time': ...  2010-11-22 08:52:43   \n",
      "3  [{'user_id': '103560060345361578357', 'time': ...  2013-08-01 09:19:36   \n",
      "4  [{'user_id': '107735448526067964417', 'time': ...  2013-08-13 10:06:50   \n",
      "\n",
      "                latest                                        review_text  \n",
      "0  2021-04-21 20:51:39  was really good.   the pizza, cheese bread and...  \n",
      "1  2021-06-18 00:16:39  really cool place.. very clean with kind peopl...  \n",
      "2  2021-04-20 00:45:20  this place has always been awesome! very frien...  \n",
      "3  2021-08-21 08:04:27  transco is always professional and great to wo...  \n",
      "4  2021-05-22 07:37:44  was there just before opening at 8:45 am.  wen...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"task1_30.json\")\n",
    "df = df.T.reset_index() \n",
    "df.columns = ['gmap_id', 'reviews', 'earliest', 'latest'] \n",
    "\n",
    "print(df.shape)\n",
    "def extract_reviews(df):\n",
    "    df = df[df['reviews'].apply(len) >= 70].reset_index(drop=True)\n",
    "    df['review_text'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        review_texts = []\n",
    "        for review in row['reviews']:\n",
    "            text = review.get('review_text', '').lower()\n",
    "            # text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "            review_texts.append(text)\n",
    "        df.at[index, 'review_text'] = ' '.join(review_texts)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "df_review = extract_reviews(df)\n",
    "print(df_review.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CJDLDI6lDEY"
   },
   "source": [
    "Let's examine what is the content of the file. For this purpose, .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyLbqkRxmCEZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpDVyW4YlDEZ"
   },
   "source": [
    "It is noteiced that file contains ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7c1-c3olDEa"
   },
   "source": [
    "Having parsed the pdf file, the following observations can be made: ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i0yElwywlDEa"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0SA37sl0lDEa"
   },
   "outputs": [],
   "source": [
    "#some code to download paper files if needed.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ENnHWjoXlDEc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9esGMx8lDEc"
   },
   "source": [
    "In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YsnRR2c4lDEc"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uh9oUXAlDEd"
   },
   "source": [
    "Let's examine the dictionary generated. For counting the total number of reviews extracted ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vUZuFeuQlDEd"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VIfQCD1VlDEe"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchByyjolDEf"
   },
   "source": [
    "Tokenization is a principal step in text processing and producing unigrams. In this section, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code\n",
    "for index,row in df_review.iterrows():\n",
    "    # Tokenization\n",
    "    review_text = row[\"review_text\"]\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    unigram_tokens = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # Remove tokens less than three\n",
    "    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n",
    "\n",
    "    # Remove independent stop words\n",
    "    stopwords_list = []\n",
    "    with open(r'Student Data/stopwords_en.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            stopwords_list.append(line.strip())\n",
    "    stopped_tokens = [w for w in unigram_tokens if w not in stopwords_list]\n",
    "\n",
    "    #Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "    df_review.at[index, \"tokennized_review\"] = stemed_tokens\n",
    "    # print(mwe_tokens)\n",
    "\n",
    "# Remove context dependent and rare word\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_context_dependent_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses > 0.95]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    cleaned_tokens = [w for w in row['tokennized_review'] if w not in rare_context_dependent_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = cleaned_tokens\n",
    "\n",
    "# Bigram words\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "bigram_token = finder.nbest(bigram_measures.pmi, 200)  # Get the top 200 bigrams\n",
    "tokenizer = MWETokenizer(bigram_token)\n",
    "for index, row in df_review.iterrows():\n",
    "    mwe_tokens = tokenizer.tokenize(row[\"tokennized_review\"])\n",
    "    df_review.at[index, \"tokennized_review\"] = mwe_tokens\n",
    "\n",
    "# Remove rare word\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses < 0.05]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    cleaned_tokens = [w for w in row['tokennized_review'] if w not in rare_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = cleaned_tokens\n",
    "    \n",
    "    \n",
    "# print(df_review.head())\n",
    "\n",
    "# Export word count txt file\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "unique_vocab = set(all_tokens)\n",
    "sorted_unique_vocab = sorted(unique_vocab)\n",
    "with open(\"test_vocab.txt\",'w') as file:\n",
    "    for index,word in enumerate(sorted_unique_vocab):\n",
    "        file.write(f\"{word}:{index}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code\n",
    "for index,row in df_review.iterrows():\n",
    "    # Tokenization\n",
    "    review_text = row[\"review_text\"]\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    unigram_tokens = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # Remove tokens less than three\n",
    "    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n",
    "\n",
    "    # Remove independent stop words\n",
    "    stopwords_list = []\n",
    "    with open(r'Student Data/stopwords_en.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            stopwords_list.append(line.strip())\n",
    "    stopped_tokens = [w for w in unigram_tokens if w not in stopwords_list]\n",
    "\n",
    "    #Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "    df_review.at[index, \"tokennized_review\"] = stemed_tokens\n",
    "    # print(mwe_tokens)\n",
    "\n",
    "# Remove context dependent and rare word\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_context_dependent_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses > 0.95 or len(business_set) / total_businesses < 0.05]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    cleaned_tokens = [w for w in row['tokennized_review'] if w not in rare_context_dependent_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = cleaned_tokens\n",
    "\n",
    "# Bigram words\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "bigram_token = finder.nbest(bigram_measures.pmi, 200)  # Get the top 200 bigrams\n",
    "tokenizer = MWETokenizer(bigram_token)\n",
    "for index, row in df_review.iterrows():\n",
    "    mwe_tokens = tokenizer.tokenize(row[\"tokennized_review\"])\n",
    "    df_review.at[index, \"tokennized_review\"] = mwe_tokens\n",
    "    \n",
    "    \n",
    "# print(df_review.head())\n",
    "\n",
    "# Export word count txt file\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "unique_vocab = set(all_tokens)\n",
    "sorted_unique_vocab = sorted(unique_vocab)\n",
    "with open(\"test_vocab.txt\",'w') as file:\n",
    "    for index,word in enumerate(sorted_unique_vocab):\n",
    "        file.write(f\"{word}:{index}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p8zT4N0RlDEf"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokennized_review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4561\u001b[0m, in \u001b[0;36mDataFrame._set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   4560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4561\u001b[0m     icol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4562\u001b[0m     iindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(index)\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokennized_review'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m     31\u001b[0m     stemed_tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m stopped_tokens]\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mdf_review\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokennized_review\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m stemed_tokens\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(df_review.head())\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Remove context dependent and rare word\u001b[39;00m\n\u001b[1;32m     39\u001b[0m word_in_business \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:2586\u001b[0m, in \u001b[0;36m_AtIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   2584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:2542\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4575\u001b[0m, in \u001b[0;36mDataFrame._set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   4573\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[index, col] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   4574\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4575\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   4576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_cache\u001b[38;5;241m.\u001b[39mpop(col, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidIndexError \u001b[38;5;28;01mas\u001b[39;00m ii_err:\n\u001b[1;32m   4579\u001b[0m     \u001b[38;5;66;03m# GH48729: Seems like you are trying to assign a value to a\u001b[39;00m\n\u001b[1;32m   4580\u001b[0m     \u001b[38;5;66;03m# row when only scalar options are permitted\u001b[39;00m\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1890\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key] \u001b[38;5;241m=\u001b[39m infer_fill_value(value)\n\u001b[1;32m   1887\u001b[0m     new_indexer \u001b[38;5;241m=\u001b[39m convert_from_missing_indexer_tuple(\n\u001b[1;32m   1888\u001b[0m         indexer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m   1889\u001b[0m     )\n\u001b[0;32m-> 1890\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;66;03m# reindex the axis\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;66;03m# make sure to clear the cache because we are\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;66;03m# just replacing the block manager here\u001b[39;00m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;66;03m# so the object is the same\u001b[39;00m\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/Monash/FIT5196/Assignment1/FIT5198-Assignment-1-/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1998\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(info_axis):\n\u001b[1;32m   1994\u001b[0m         \u001b[38;5;66;03m# This is a case like df.iloc[:3, [1]] = [0]\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m         \u001b[38;5;66;03m#  where we treat as df.iloc[:3, 1] = 0\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer((pi, info_axis[\u001b[38;5;241m0\u001b[39m]), value[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1998\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1999\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen setting with an iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2001\u001b[0m     )\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# We get here in one case via .loc with a all-False mask\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "#some code\n",
    "for index,row in df_review.iterrows():\n",
    "    # Tokenization\n",
    "    review_text = row[\"review_text\"]\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    unigram_tokens = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # Remove tokens less than three\n",
    "    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n",
    "\n",
    "    # Retokenize for multi word\n",
    "    bigrams = ngrams(unigram_tokens, n = 2)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "    fdbigram.most_common()\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(unigram_tokens)\n",
    "    bigram_token = finder.nbest(bigram_measures.pmi, 201)\n",
    "    tokenizer = MWETokenizer(bigram_token)\n",
    "    mwe_tokens = tokenizer.tokenize(unigram_tokens)\n",
    "    # print(mwe_tokens)\n",
    "\n",
    "    # Remove independent stop words\n",
    "    stopwords_list = []\n",
    "    with open(r'Student Data/stopwords_en.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            stopwords_list.append(line.strip())\n",
    "    stopped_tokens = [w for w in mwe_tokens if w not in stopwords_list]\n",
    "\n",
    "    # Stem words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "\n",
    "    df_review.at[index, \"tokennized_review\"] = stemed_tokens\n",
    "\n",
    "\n",
    "# print(df_review.head())\n",
    "\n",
    "# Remove context dependent and rare word\n",
    "word_in_business = {}\n",
    "total_businesses = df_review.shape[0]\n",
    "\n",
    "for index, row in df_review.iterrows():\n",
    "    gmap_id = row['gmap_id']\n",
    "    unique_words = set(row['tokennized_review'])\n",
    "    for word in unique_words:\n",
    "        if word not in word_in_business:\n",
    "            word_in_business[word] = set()\n",
    "        word_in_business[word].add(gmap_id)\n",
    "\n",
    "rare_context_dependent_stopwords = [word for word, business_set in word_in_business.items()\n",
    "                                if len(business_set) / total_businesses > 0.95 or len(business_set) / total_businesses < 0.05]\n",
    "\n",
    "for index,row in df_review.iterrows():\n",
    "    cleaned_tokens = [w for w in row['tokennized_review'] if w not in rare_context_dependent_stopwords]\n",
    "    df_review.at[index, \"tokennized_review\"] = cleaned_tokens\n",
    "\n",
    "# print(df_review.head())\n",
    "\n",
    "# Export word count txt file\n",
    "all_tokens = sum(df_review[\"tokennized_review\"], [])\n",
    "unique_vocab = set(all_tokens)\n",
    "sorted_unique_vocab = sorted(unique_vocab)\n",
    "with open(\"test_vocab.txt\",'w') as file:\n",
    "    for index,word in enumerate(sorted_unique_vocab):\n",
    "        file.write(f\"{word}:{index}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZos1q6lDEf"
   },
   "source": [
    "The above operation results in a dictionary with PID representing keys and a single string for all reviews of the day concatenated to each other. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPBNTTq6lDEg"
   },
   "outputs": [],
   "source": [
    "#Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVqFfwwMlDEg"
   },
   "source": [
    "At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NglwwiJRnPZd"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.2. Whatever else <a class=\"anchor\" name=\"whetev\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ve6IZ2I-lDEg"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.3. Generate numerical representation<a class=\"anchor\" name=\"bigrams\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGhUY2UlDEg"
   },
   "source": [
    "One of the tasks is to generate the numerical representation for all tokens in abstract.  ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgFFtm6qlDEg"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpy1-tQalDEg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cbSKT6PlDEg"
   },
   "source": [
    "Random descriptions and justification ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hDrmwEylDEg"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO1PJO-dlDEh"
   },
   "source": [
    "At this stage, we have a dictionary of tokenized words, whose keys are indicative of....\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfMYJ0XdlDEh"
   },
   "source": [
    "#### Whatever else <a class=\"anchor\" name=\"whatev1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmaGJYIJlDEl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMBqRetlDEl"
   },
   "source": [
    "files need to be generated:\n",
    "* Vocabulary list\n",
    "* Sparse matrix (count_vectors)\n",
    "\n",
    "This is performed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc6tQ4ljlDEm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDlbpGYilDEm"
   },
   "source": [
    "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6OUXHlxlDEm"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkGH81YFlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtxqUAwmlDEn"
   },
   "source": [
    "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__n1fdIqlDEn"
   },
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUFQU-QXlDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWjri6x_lDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAprlSblDEn"
   },
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXYKxO8lDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HppxDtWNlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCkWr-M1lDEo"
   },
   "source": [
    "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9O-a1UlDEo"
   },
   "source": [
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
